{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ee9c07d",
   "metadata": {},
   "source": [
    "![PNG%EC%9D%B4~3.PNG](https://user-images.githubusercontent.com/75057952/167243294-0400b833-5bbd-46ce-adc5-bcd1073969df.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "410c520d",
   "metadata": {},
   "source": [
    "# PixelCNN++ : IMPROVING THE PIXELCNN WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS \n",
    "Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma\n",
    "### Recap - **PixelCNN : Generative model of images with a tractable likelihood**\n",
    "- Fully factorizes the probability density function on an image x over all its sub-pixels (color channels in a pixel)\n",
    "- $p(\\mathcal x) = \\Pi_i p(x_i|X_{<i})$ : conditional distributions $ p(x_i|X_{<i}$) are parameterized by CNNs\n",
    "---\n",
    "### Modifications to PixelCNN\n",
    "**1. Discretized logistic mixture likelihood**\n",
    "\n",
    "**PixelCNN**\n",
    "- PixelCNN model에서는 pixel의 color-channel 단위의 conditional distribution으로 분해함\n",
    "- Probability distribution은 256-way(0~255) softmax로 구해지기 때문에 계산량이 많음(Regression Problem > 256 classification problem)\n",
    "- 굉장히 Sparse한 gradient : early training step에서 특히 두드러짐\n",
    "- 127과 128은 difference class로 인식, numerically adjacent하다는 것을 알지 못함\n",
    "---\n",
    "**PixelCNN++**\n",
    "- VAE와 유사하게 Pixel value의 conditional probability를 설명하는 latent variable $\\nu$의 분포를 추정하는 것이 목표\n",
    "- $\\nu$의 분포로 여기서는 logistic distribution을 가정\n",
    "- $\\nu \\sim \\sum_{i=1}^K \\pi_i \\text{logistic}(\\mu_i, s_i)$\n",
    "- $P(x|\\pi, \\mu, s) = \\sum_{i=1}^K \\pi_i[\\sigma({(x+0.5-\\mu_i) \\over s_i}) - \\sigma({(x-0.5-\\mu_i) \\over s_i})]$ (edge case 제외)\n",
    "- CIFAR-10 데이터셋의 pixel conditional distribution plot\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/167244052-6db2a090-c151-42a0-805c-bde66e81fb81.png\" width = \"500dp\"></img>\n",
    "---\n",
    "**2. Conditioning on whole pixels**\n",
    "\n",
    "**PixelCNN**\n",
    "- R > G > B 순서대로 channel 우선순위를 정하고, channel 단위로 분해해서 conditional distribution을 구함\n",
    "- 하나의 pixel이 가지는 3개의 sub-pixel에 대한 generative model을 세움\n",
    "- General한 dependency를 설명하기에는 적합하지만 너무 complicated\n",
    "---\n",
    "**PixelCNN++**\n",
    "- 가정 : Inter-channel dependency는 복잡한 relationship이 아님\n",
    "- Simple factorized model로도 충분히 설명 가능하다고 생각\n",
    "- $C_{i,j}$ : context vector(이전 픽셀들의 정보, mixture indicator across all 3 channels)\n",
    "- $p(r_{i,j}, g_{i,j}, b_{i,j} | C_{i,j}) = P(r_{i,j}|\\mu_r(C_{i,j}), s_r(C_{i,j})) \\times P(g_{i,j}|\\mu_g(C_{i,j}, r_{i,j}), s_g(C_{i,j})) \\times P(b_{i,j}|\\mu_b(C_{i,j}, r_{i,j}, g_{i,j}), s_b(C_{i,j}))$\n",
    "- $\\mu_g{(C_{i,j}, r_{i,j})} = \\mu_g{(C_{i,j})} + \\alpha(C_{i,j}) r_{i,j}$\n",
    "- $\\mu_b(C_{i,j}, r_{i,j}, g_{i,j} = \\mu_b(C_{i,j}) + \\beta(C_{i,j})r_{i,j} + \\gamma(C_{i,j})b_{i,j}$\n",
    "\n",
    "---\n",
    "**3.  Downsampling vs. dilated convolution**\n",
    "\n",
    "**PixelCNN**\n",
    "- Small receptive field의 convolution filter(주로 3X3)을 사용\n",
    "- Local dependency capture에는 용이하지만 long-range structure modeling에는 부적합\n",
    "---\n",
    "**PixelCNN++**\n",
    "- Receptive field를 크게 하기 위해서 Dilated convolution으로 multi-resolution view\n",
    "- 혹은 downsampling 이후에 convolution을 하기도 함\n",
    "    - loses information\n",
    "    - compensate by introducing additional short-cut connections\n",
    "    - 결과론적으로는 dilated convolution과 큰 성능 차이 X\n",
    "---\n",
    "**4.  Adding short-cut connections**\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/167244057-a190e3de-84e6-4ca7-bb0f-a44e448d3737.png\" width = \"700dp\"></img>\n",
    "- U-Net like architecture\n",
    "- 총 6개의 layer로 구성되어 있는데, symmetric한 추상화 계층에 대해서 convolutional connection으로 연결\n",
    "- Skip connection compensates information loss during downsampling, upsampling process\n",
    "\n",
    "**5. Regularization using dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89945adc",
   "metadata": {},
   "source": [
    "### Experiments (and Ablation Study)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5340edd",
   "metadata": {},
   "source": [
    "**Unconditional generation on CIFAR-10 : negative log-likelihood**\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/167244061-c205d93e-bcee-46e7-9f16-6f7cc43662af.png\" width = \"300dp\"></img>\n",
    "---\n",
    "**Network depth and field of view size**\n",
    "- PixelCNN Experiment을 보고 저자들이 세운 가설 : Receptive field size, removal of blind spots가 성능에 영향을 크게 미칠 것\n",
    "- PixelCNN ++에서 Layer를 deep하지 않게 쌓거나 Receptive field size를 제한하면 > Network capacity가 떨어짐에도 불구하고 PixelCNN보다 나음(Plain)\n",
    "- Plain에서 modification :  NIN(Network in Network), Autoregressive Channel을 도입(Capcity와 성능 측면 향상)\n",
    "    - NIN : <img src = \"https://user-images.githubusercontent.com/75057952/167244066-88d40748-6444-4eec-8108-c97e7169d283.png\" width = \"300dp\"></img>\n",
    "    - Autoregressive Channel : channel 사이의 skip connection(1X1 convolution gated ResNet block)를 도입\n",
    "<img src = \"https://user-images.githubusercontent.com/75057952/167244068-4165cf0a-5c3a-4192-82f1-ce3c03acd8c5.png\" width = \"300dp\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "import pixel_cnn_pp.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c19ada1",
   "metadata": {},
   "source": [
    "![image.png](https://user-images.githubusercontent.com/75057952/167244074-db550f12-4875-4796-9d94-3256d36f546f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_spec(x, h=None, init=False, ema=None, dropout_p=0.5, nr_resnet=5, nr_filters=160, nr_logistic_mix=10, resnet_nonlinearity='concat_elu', energy_distance=False):\n",
    "\n",
    "    counters = {}\n",
    "    with arg_scope([nn.conv2d, nn.deconv2d, nn.gated_resnet, nn.dense], counters=counters, init=init, ema=ema, dropout_p=dropout_p):\n",
    "\n",
    "        # parse resnet nonlinearity argument\n",
    "        if resnet_nonlinearity == 'concat_elu':\n",
    "            resnet_nonlinearity = nn.concat_elu\n",
    "        elif resnet_nonlinearity == 'elu':\n",
    "            resnet_nonlinearity = tf.nn.elu\n",
    "        elif resnet_nonlinearity == 'relu':\n",
    "            resnet_nonlinearity = tf.nn.relu\n",
    "        else:\n",
    "            raise('resnet nonlinearity ' + resnet_nonlinearity + ' is not supported')\n",
    "\n",
    "        with arg_scope([nn.gated_resnet], nonlinearity=resnet_nonlinearity, h=h):\n",
    "\n",
    "            # ////////// up pass through pixelCNN ////////\n",
    "            xs = nn.int_shape(x)\n",
    "            '''\n",
    "            def int_shape(x):\n",
    "                return list(map(int, x.get_shape()))\n",
    "            '''\n",
    "            x_pad = tf.concat([x,tf.ones(xs[:-1]+[1])],3) # add channel of ones to distinguish image from padding later on\n",
    "            '''\n",
    "            def down_shift(x):\n",
    "                xs = int_shape(x)\n",
    "                return tf.concat([tf.zeros([xs[0],1,xs[2],xs[3]]), x[:,:xs[1]-1,:,:]],1)\n",
    "            '''\n",
    "            u_list = [nn.down_shift(nn.down_shifted_conv2d(x_pad, num_filters=nr_filters, filter_size=[2, 3]))] # stream for pixels above\n",
    "            ul_list = [nn.down_shift(nn.down_shifted_conv2d(x_pad, num_filters=nr_filters, filter_size=[1,3])) + \\\n",
    "                       nn.right_shift(nn.down_right_shifted_conv2d(x_pad, num_filters=nr_filters, filter_size=[2,1]))] # stream for up and to the left\n",
    "            '''\n",
    "            down_shift : input의 above에 zero layer를 하나 추가\n",
    "            down_shifted_conv2d/deconv2d : \n",
    "            def down_shifted_conv2d(x, num_filters, filter_size=[2,3], stride=[1,1], **kwargs):\n",
    "                # NCHW 축으로의 padding 값을 넣어줌. H, W 축으로만 shift 해줌.\n",
    "                x = tf.pad(x, [[0,0],[filter_size[0]-1,0], [int((filter_size[1]-1)/2),int((filter_size[1]-1)/2)],[0,0]])\n",
    "                return conv2d(x, num_filters, filter_size=filter_size, pad='VALID', stride=stride, **kwargs)\n",
    "            '''\n",
    "            for rep in range(nr_resnet): # nr_resnet = 5 (default)\n",
    "                u_list.append(nn.gated_resnet(u_list[-1], conv=nn.down_shifted_conv2d))\n",
    "                ul_list.append(nn.gated_resnet(ul_list[-1], u_list[-1], conv=nn.down_right_shifted_conv2d))\n",
    "            '''\n",
    "            gated_resnet : 위에 그림 참조\n",
    "            '''\n",
    "            u_list.append(nn.down_shifted_conv2d(u_list[-1], num_filters=nr_filters, stride=[2, 2]))\n",
    "            ul_list.append(nn.down_right_shifted_conv2d(ul_list[-1], num_filters=nr_filters, stride=[2, 2]))\n",
    "\n",
    "            for rep in range(nr_resnet):\n",
    "                u_list.append(nn.gated_resnet(u_list[-1], conv=nn.down_shifted_conv2d))\n",
    "                ul_list.append(nn.gated_resnet(ul_list[-1], u_list[-1], conv=nn.down_right_shifted_conv2d))\n",
    "\n",
    "            u_list.append(nn.down_shifted_conv2d(u_list[-1], num_filters=nr_filters, stride=[2, 2]))\n",
    "            ul_list.append(nn.down_right_shifted_conv2d(ul_list[-1], num_filters=nr_filters, stride=[2, 2]))\n",
    "\n",
    "            for rep in range(nr_resnet):\n",
    "                u_list.append(nn.gated_resnet(u_list[-1], conv=nn.down_shifted_conv2d))\n",
    "                ul_list.append(nn.gated_resnet(ul_list[-1], u_list[-1], conv=nn.down_right_shifted_conv2d))\n",
    "\n",
    "            # remember nodes\n",
    "            for t in u_list+ul_list:\n",
    "                tf.add_to_collection('checkpoints', t)\n",
    "\n",
    "            # /////// down pass ////////\n",
    "            u = u_list.pop()\n",
    "            ul = ul_list.pop()\n",
    "            for rep in range(nr_resnet):\n",
    "                u = nn.gated_resnet(u, u_list.pop(), conv=nn.down_shifted_conv2d)\n",
    "                ul = nn.gated_resnet(ul, tf.concat([u, ul_list.pop()],3), conv=nn.down_right_shifted_conv2d)\n",
    "                tf.add_to_collection('checkpoints', u)\n",
    "                tf.add_to_collection('checkpoints', ul)\n",
    "\n",
    "            u = nn.down_shifted_deconv2d(u, num_filters=nr_filters, stride=[2, 2])\n",
    "            ul = nn.down_right_shifted_deconv2d(ul, num_filters=nr_filters, stride=[2, 2])\n",
    "\n",
    "            for rep in range(nr_resnet+1):\n",
    "                u = nn.gated_resnet(u, u_list.pop(), conv=nn.down_shifted_conv2d)\n",
    "                ul = nn.gated_resnet(ul, tf.concat([u, ul_list.pop()],3), conv=nn.down_right_shifted_conv2d)\n",
    "                tf.add_to_collection('checkpoints', u)\n",
    "                tf.add_to_collection('checkpoints', ul)\n",
    "\n",
    "            u = nn.down_shifted_deconv2d(u, num_filters=nr_filters, stride=[2, 2])\n",
    "            ul = nn.down_right_shifted_deconv2d(ul, num_filters=nr_filters, stride=[2, 2])\n",
    "\n",
    "            for rep in range(nr_resnet+1):\n",
    "                u = nn.gated_resnet(u, u_list.pop(), conv=nn.down_shifted_conv2d)\n",
    "                ul = nn.gated_resnet(ul, tf.concat([u, ul_list.pop()],3), conv=nn.down_right_shifted_conv2d)\n",
    "                tf.add_to_collection('checkpoints', u)\n",
    "                tf.add_to_collection('checkpoints', ul)\n",
    "\n",
    "                '''\n",
    "                nin : network in network\n",
    "                '''\n",
    "            if energy_distance:\n",
    "                f = nn.nin(tf.nn.elu(ul), 64)\n",
    "\n",
    "                # generate 10 samples\n",
    "                fs = []\n",
    "                for rep in range(10):\n",
    "                    fs.append(f)\n",
    "                f = tf.concat(fs, 0)\n",
    "                fs = nn.int_shape(f)\n",
    "                f += nn.nin(tf.random_uniform(shape=fs[:-1] + [4], minval=-1., maxval=1.), 64)\n",
    "                f = nn.nin(nn.concat_elu(f), 64)\n",
    "                x_sample = tf.tanh(nn.nin(nn.concat_elu(f), 3, init_scale=0.1))\n",
    "\n",
    "                x_sample = tf.split(x_sample, 10, 0)\n",
    "\n",
    "                assert len(u_list) == 0\n",
    "                assert len(ul_list) == 0\n",
    "\n",
    "                return x_sample\n",
    "\n",
    "            else:\n",
    "                x_out = nn.nin(tf.nn.elu(ul),10*nr_logistic_mix)\n",
    "\n",
    "                assert len(u_list) == 0\n",
    "                assert len(ul_list) == 0\n",
    "\n",
    "                return x_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
