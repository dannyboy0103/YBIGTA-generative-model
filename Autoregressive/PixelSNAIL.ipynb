{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PixelSNAIL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **PixelSNAIL: An Improved Autoregressive Generative Model 리뷰**"
      ],
      "metadata": {
        "id": "mnPiVTFD2osG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abstract\n",
        "\n",
        "Autoregressive Generative Model → best results in high dimensional data \n",
        "\n",
        "ex) images or audio\n",
        "\n",
        "as a sequence modeling task\n",
        "\n",
        "deal with long-range dependency → ***casual convolutions with self-attention***\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Autoregressive Generative Model은 보통 다음 식과 같이 표현될 수 있음\n",
        "\n",
        "![joint distribution as a product of conditionals](https://user-images.githubusercontent.com/66329748/178414558-6cf8bb1c-2125-43e4-80ea-bcf3364ab850.png)\n",
        "\n",
        "joint distribution as a product of conditionals\n",
        "\n",
        "GAN과 비교했을 때, Neural Autoregressive Model은\n",
        "\n",
        "- offer tractable likelihood computation\n",
        "- ease of training\n",
        "- outperform latent variable models\n",
        "\n",
        "Main Design Consideration\n",
        "\n",
        "must be able to easily refer to earlier parts of the sequence\n",
        "\n",
        "1. Traditional RNNs\n",
        "    \n",
        "    정보들을 hidden state에 담아 다음 timestep으로 전달\n",
        "    \n",
        "    data의 long-range relationship에 방해 (-)\n",
        "    \n",
        "2. Casual Convolutions\n",
        "    \n",
        "    current prediction is only influenced by previous element\n",
        "    \n",
        "    high-bandwidth access to the earlier parts of the sequence (+)\n",
        "    \n",
        "    finite size of receptive field (-)\n",
        "    \n",
        "    sequence에서 먼 거리의 element로부터는 전달 감쇠 (-)\n",
        "    \n",
        "3. Self-Attention\n",
        "    \n",
        "    unbounded receptive field (+)\n",
        "    \n",
        "    sequence에서 먼 거리의 element에도 undeteriorated access (+)\n",
        "    \n",
        "    pinpoint access to small amounts of information (-)\n",
        "    \n",
        "    positional information 사용하려면 additional mechanism 필요 (-)\n",
        "    \n",
        "\n",
        "***Casual Convolution과 Self-Attention의 장단점이 서로 보완해주는 점***\n",
        "\n",
        "Casual Convolution : high-bandwidth access over a finite context size (가까운 거리만)\n",
        "\n",
        "Self-Attention : access over an infinitely large context (하지만 pinpoint access to small amount of information)\n",
        "\n",
        "***Interleaving → high-bandwidth access without constraints on the amount of information it can effectively use***\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "![스크린샷 2022-07-07 오후 7.44.02.png](https://user-images.githubusercontent.com/66329748/178414679-7a6a7710-3aea-480e-a6a1-8b49a670ffe7.png)\n",
        "\n",
        "![스크린샷 2022-07-08 오후 11.11.31.png](https://user-images.githubusercontent.com/66329748/178414723-09a63fef-3b18-4dd5-92e1-8a282aa82b13.png)\n",
        "\n",
        "(a) residual block \n",
        "\n",
        "masked convolutions → current pixel은 왼쪽 위의 pixel만 access\n",
        "\n",
        "![Untitled](https://user-images.githubusercontent.com/66329748/178414765-bea9cc64-c548-4039-bb69-5640205e4c32.png)\n",
        "\n",
        "(b) attention block\n",
        "\n",
        "project input to lower dimensionality to produce keys & values\n",
        "\n",
        "## Comparison\n",
        "\n",
        "![Untitled](https://user-images.githubusercontent.com/66329748/178414823-58cc95a9-ca3b-4d9a-b1f5-1002a7e7d8f4.png)\n",
        "\n",
        "![Untitled](https://user-images.githubusercontent.com/66329748/178414958-65f1b1d1-47ea-471e-95be-781cd4c22eb8.png)\n",
        "\n",
        "![Untitled](https://user-images.githubusercontent.com/66329748/178414995-78cbe6f1-1227-42ca-b353-4abafaf5344b.png)\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Autoregressive Generative Model\n",
        "\n",
        "tractable likelihood (+)\n",
        "\n",
        "strong empirical performance (+)\n",
        "\n",
        "slow sampling (-)"
      ],
      "metadata": {
        "id": "lTahYU1B0b3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PixelSNAIL 구현"
      ],
      "metadata": {
        "id": "Za_IGs5A0uqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import distributions\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from pytorch_generative import nn as pg_nn\n",
        "from pytorch_generative.models import base"
      ],
      "metadata": {
        "id": "ra-FQZAKVGNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _elu_conv_elu(conv, x):\n",
        "    return F.elu(conv(F.elu(x)))\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with a gated activation function.\"\"\"\n",
        "\n",
        "    def __init__(self, n_channels):\n",
        "        \"\"\"Initializes a new ResidualBlock.\n",
        "        Args:\n",
        "            n_channels: The number of input and output channels.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._input_conv = nn.Conv2d(\n",
        "            in_channels=n_channels, out_channels=n_channels, kernel_size=2, padding=1\n",
        "        )\n",
        "        self._output_conv = nn.Conv2d(\n",
        "            in_channels=n_channels,\n",
        "            out_channels=2 * n_channels,\n",
        "            kernel_size=2,\n",
        "            padding=1,\n",
        "        )\n",
        "        self._activation = pg_nn.GatedActivation(activation_fn=nn.Identity())\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, c, h, w = x.shape\n",
        "        out = _elu_conv_elu(self._input_conv, x)[:, :, :h, :w]\n",
        "        out = self._activation(self._output_conv(out)[:, :, :h, :w]) #elu conv elu 후에 찢어서 한쪽만 sigmoid하고 다시 elementwise mul하는 과정을 생략한듯. 바로 Input과 +\n",
        "        return x + out"
      ],
      "metadata": {
        "id": "V2Ada7mhVGQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelSNAILBlock(nn.Module):\n",
        "    \"\"\"Block comprised of a number of residual blocks plus one attention block.\n",
        "    Implements Figure 5 of [1].\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels,\n",
        "        input_img_channels=1,\n",
        "        n_residual_blocks=2,\n",
        "        attention_key_channels=4,\n",
        "        attention_value_channels=32,\n",
        "    ):\n",
        "        \"\"\"Initializes a new PixelSnailBlock instance.\n",
        "        Args:\n",
        "            n_channels: Number of input and output channels.\n",
        "            input_img_channels: The number of channels in the original input_img. Used\n",
        "                for the positional encoding channels and the extra channels for the key\n",
        "                and value convolutions in the attention block.\n",
        "            n_residual_blocks: Number of residual blocks.\n",
        "            attention_key_channels: Number of channels (dims) for the attention key.\n",
        "            attention_value_channels: Number of channels (dims) for the attention value.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        def conv(in_channels):\n",
        "            return nn.Conv2d(in_channels, out_channels=n_channels, kernel_size=1)\n",
        "\n",
        "        self._residual = nn.Sequential(\n",
        "            *[ResidualBlock(n_channels) for _ in range(n_residual_blocks)] # Figure1의 (a)보면 residual block이 R번 반복됨\n",
        "        )\n",
        "        self._attention = pg_nn.CausalAttention(\n",
        "            in_channels=n_channels + 2,\n",
        "            embed_channels=attention_key_channels,\n",
        "            out_channels=attention_value_channels,\n",
        "            mask_center=True,\n",
        "            extra_input_channels=input_img_channels,\n",
        "        )\n",
        "        self._residual_out = conv(n_channels)\n",
        "        self._attention_out = conv(attention_value_channels)\n",
        "        self._out = conv(n_channels)\n",
        "\n",
        "    def forward(self, x, input_img):\n",
        "        \"\"\"Computes the forward pass.\n",
        "        Args:\n",
        "            x: The input.\n",
        "            input_img: The original image only used as input to the attention blocks.\n",
        "        Returns:\n",
        "            The result of the forward pass.\n",
        "        \"\"\"\n",
        "        res = self._residual(x)\n",
        "        pos = pg_nn.image_positional_encoding(input_img.shape).to(res.device) # attention 초기 작업\n",
        "        \"\"\"Generates positional encodings for 2d images.\n",
        "        The positional encoding is a Tensor of shape (N, 2, H, W) of (x, y) pixel\n",
        "        coordinates scaled to be between -.5 and .5.\n",
        "        Args:\n",
        "            shape: NCHW shape of image for which to generate positional encodings.\n",
        "        Returns:\n",
        "            The positional encodings.\n",
        "        \"\"\"\n",
        "        attn = self._attention(torch.cat((pos, res), dim=1), input_img)\n",
        "        res, attn = (\n",
        "            _elu_conv_elu(self._residual_out, res),\n",
        "            _elu_conv_elu(self._attention_out, attn),\n",
        "        )\n",
        "        return _elu_conv_elu(self._out, res + attn)"
      ],
      "metadata": {
        "id": "vPFuMKI3VGVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelSNAIL(base.AutoregressiveModel):\n",
        "    \"\"\"The PixelSNAIL model.\n",
        "    Unlike [1], we implement skip connections from each block to the output.\n",
        "    We find that this makes training a lot more stable and allows for much faster\n",
        "    convergence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=1,\n",
        "        out_channels=1,\n",
        "        n_channels=64,\n",
        "        n_pixel_snail_blocks=8,\n",
        "        n_residual_blocks=2,\n",
        "        attention_key_channels=4,\n",
        "        attention_value_channels=32,\n",
        "        sample_fn=None,\n",
        "    ):\n",
        "        \"\"\"Initializes a new PixelSNAIL instance.\n",
        "        Args:\n",
        "            in_channels: Number of input channels.\n",
        "            out_channels: Number of output_channels.\n",
        "            n_channels: Number of channels to use for convolutions.\n",
        "            n_pixel_snail_blocks: Number of PixelSNAILBlocks.\n",
        "            n_residual_blocks: Number of ResidualBlock to use in each PixelSnailBlock.\n",
        "            attention_key_channels: Number of channels (dims) for the attention key.\n",
        "            attention_value_channels: Number of channels (dims) for the attention value.\n",
        "            sample_fn: See the base class.\n",
        "        \"\"\"\n",
        "        super().__init__(sample_fn)\n",
        "        self._input = pg_nn.CausalConv2d(\n",
        "            mask_center=True,\n",
        "            in_channels=in_channels,\n",
        "            out_channels=n_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "        )\n",
        "        self._pixel_snail_blocks = nn.ModuleList(\n",
        "            [\n",
        "                PixelSNAILBlock(\n",
        "                    n_channels=n_channels,\n",
        "                    input_img_channels=in_channels,\n",
        "                    n_residual_blocks=n_residual_blocks,\n",
        "                    attention_key_channels=attention_key_channels,\n",
        "                    attention_value_channels=attention_value_channels,\n",
        "                )\n",
        "                for _ in range(n_pixel_snail_blocks)\n",
        "            ]\n",
        "        )\n",
        "        self._output = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=n_channels, out_channels=n_channels // 2, kernel_size=1\n",
        "            ),\n",
        "            nn.Conv2d(\n",
        "                in_channels=n_channels // 2, out_channels=out_channels, kernel_size=1\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_img = x\n",
        "        x = self._input(x)\n",
        "        for block in self._pixel_snail_blocks:\n",
        "            x = x + block(x, input_img) # skip connection. 논문과는 조금 다른 구현\n",
        "        return self._output(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ue5cgFi1VGaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reproduce(\n",
        "    n_epochs=457,\n",
        "    batch_size=128,\n",
        "    log_dir=\"/tmp/run\",\n",
        "    n_gpus=1,\n",
        "    device_id=0,\n",
        "    debug_loader=None,\n",
        "):\n",
        "    \"\"\"Training script with defaults to reproduce results.\n",
        "    The code inside this function is self contained and can be used as a top level\n",
        "    training script, e.g. by copy/pasting it into a Jupyter notebook.\n",
        "    Args:\n",
        "        n_epochs: Number of epochs to train for.\n",
        "        batch_size: Batch size to use for training and evaluation.\n",
        "        log_dir: Directory where to log trainer state and TensorBoard summaries.\n",
        "        n_gpus: Number of GPUs to use for training the model. If 0, uses CPU.\n",
        "        device_id: The device_id of the current GPU when training on multiple GPUs.\n",
        "        debug_loader: Debug DataLoader which replaces the default training and\n",
        "            evaluation loaders if not 'None'. Do not use unless you're writing unit\n",
        "            tests.\n",
        "    \"\"\"\n",
        "    from torch import optim\n",
        "    from torch.nn import functional as F\n",
        "    from torch.optim import lr_scheduler\n",
        "\n",
        "    from pytorch_generative import datasets\n",
        "    from pytorch_generative import models\n",
        "    from pytorch_generative import trainer\n",
        "\n",
        "    train_loader, test_loader = debug_loader, debug_loader\n",
        "    if train_loader is None:\n",
        "        train_loader, test_loader = datasets.get_mnist_loaders(\n",
        "            batch_size, dynamically_binarize=True\n",
        "        )\n",
        "\n",
        "    model = models.PixelSNAIL(\n",
        "        in_channels=1,\n",
        "        out_channels=1,\n",
        "        n_channels=64,\n",
        "        n_pixel_snail_blocks=8,\n",
        "        n_residual_blocks=2,\n",
        "        attention_value_channels=32,  # n_channels / 2\n",
        "        attention_key_channels=4,  # attention_value_channels / 8\n",
        "    )\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lambda _: 0.999977)\n",
        "\n",
        "    def loss_fn(x, _, preds):\n",
        "        batch_size = x.shape[0]\n",
        "        x, preds = x.view((batch_size, -1)), preds.view((batch_size, -1))\n",
        "        loss = F.binary_cross_entropy_with_logits(preds, x, reduction=\"none\")\n",
        "        return loss.sum(dim=1).mean()\n",
        "\n",
        "    trainer = trainer.Trainer(\n",
        "        model=model,\n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        eval_loader=test_loader,\n",
        "        lr_scheduler=scheduler,\n",
        "        log_dir=log_dir,\n",
        "        n_gpus=n_gpus,\n",
        "        device_id=device_id,\n",
        "    )\n",
        "    trainer.interleaved_train_and_eval(n_epochs)"
      ],
      "metadata": {
        "id": "CCFQrGyKVGe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7clegZYUVGjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "upJOA1IvVGnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PWiZaoTqVGrw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}