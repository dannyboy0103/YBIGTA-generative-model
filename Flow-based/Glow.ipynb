{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0fvK3Rgac3X"
   },
   "source": [
    "# Review: Glow: Generative Flow with Invertible 1×1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jvt_q5jKL2y"
   },
   "source": [
    "## Paper Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS5vzHkTbVkn"
   },
   "source": [
    "### Prior Reseach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flow-based generative models**는 non-parametric density estimation의 일종으로 제안된 **normalizing flow**를 기반으로 하는 generative models임\n",
    "+ density estimation의 일종으로서 variational inference는 기본적으로 lower bound에 대한 approximation이기 때문에 true distribution에 대한 tight estimation이 불가능함\n",
    "+ 그러나 만약에 target density가 **적절한 $K$개의 transform의 chain이 존재하여** 간단한 distribution에서의 mapping으로 표현할 수 있다면 explicit하게 표현할 방법이 존재함  \n",
    "+ chain을 구성하는 각각의 transform이 **invertible**하다면 흔히 [**LOTUS**](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician)라 불리는 Theorem을 사용하는 것을 통해 target density를 몰라도 정확한 expectation을 계산해낼 수 있음  \n",
    "\n",
    "$$ \\mathbb{E}_{\\mathbf{z_K} \\sim q_K}[h(\\mathbf{z_K})] = \\mathbb{E}_{\\mathbf{z_0} \\sim q_0}[h(f_K \\circ f_{K-1} \\circ \\cdots f_1(\\mathbf{z}_0))] $$\n",
    "\n",
    "→ $K$ chain이 infinite sequence고 **Langevin SDE**를 따르면 score function $ -\\nabla_{\\mathbf{z}} \\mathcal{L}(\\mathbf{z}) $로 stationary solution의 존재를 보장할 수 있음 ( = **diffusion model**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce03Q4Eit6p-"
   },
   "source": [
    "이러한 approach는 기존의 dominant한 generative models(GAN, VAE, Autoregressive)에 비교해 다음과 같은 장점을 지니고 있음\n",
    "+ explicit하게 log-likelihood을 통한 latent space에서부터의 target density의 inference를 수행하므로 VAE보다 정확한 inference가 가능해짐\n",
    "+ Autoregressive와 같이 순차적으로 생성하는 경우보다 sampling speed가 유리함\n",
    "+ semantic을 파악하기 힘든 marginal distribution을 생성하는 Autoregressive나 latent의 해석이 어려운 GAN보다 direct하게 latent space 상에서의 이해가 가능함\n",
    "+ 특성상 Jacobian의 계산이 편해야 하는 구조이기 때문에 gradient의 계산이 쉬워 memory capacity 측면에서 다른 model보다 유리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXzIbXJXxdqe"
   },
   "source": [
    "직접적인 model architecture 측면에서의 이를 위한 대표적인 선행연구는 **RealNVP**가 있음  \n",
    "RealNVP는 **affine coupling layer**를 도입하는 것을 통해서 normalizing flow로서 만족하기 위한 invertible mapping 및 간단한 Jacobian을 얻어낼 수 있었음  \n",
    "\n",
    "$$ y_{1:d} = x_{1:d} $$\n",
    "$$ y_{d+1:D} = x_{d+1 : D} \\odot \\exp{(s(x_{1:d}))} + t(x_{1:d}) $$\n",
    "\n",
    "그러나 단순히 affine coupling layer를 사용하면 identity mapping을 수행하는 channel에 대해서 transform이 이루어지지 않음  \n",
    "따라서 RealNVP는 직접적으로 shuffling을 수행하는 과정을 추가하여 각 channel에 대한 pattern 변화를 반드시 도입했어야 했음  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/86907286/181778517-f45945fa-bc87-4519-a0a4-04de9d4df6b0.png\" alt=\"1\" width=\"300px\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 측면을 개선하여 더 좋은 mapping을 학습할 수 있도록 **learnable shuffle**을 도입한 것이 **Glow**  \n",
    "RealNVP에 비해서 image generation에서 훨씬 개선된 negative log-likelihood를 확인할 수 있었음  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/86907286/181778523-6b47e380-787a-4fa1-8a3a-9c19942caa74.png\" alt=\"2\" width=\"500px\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b06BfxUmyiyZ"
   },
   "source": [
    "### Invertible 1×1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glow에서 제시하는 learnable shuffling은 1×1 Convolutions이 **일종의 일반화된 permutation을 수행하는 것**이라는 점을 주장함  \n",
    "+ 만약 1×1 Convolutions layer의 input/output channel의 dimension이 같다면 각 input channel의 일부를 linear combination하여 output의 크기 만큼의 channel로 반환함  \n",
    "+ input channel의 order를 바꾸는 permutation을 넘어서 **각 input channel을 적절히 mixing하여 shuffling하여 같은 dimension의 output으로 반환하는 permutation**인 것  \n",
    "+ 이러한 1×1 Convolutions shuffling으로 기존의 shuffling을 대체한다고 해도 충분히 쉽게 Jacobian을 구할 수 있으며 learnable하기까지 함  \n",
    "+ 만약 $ c $ channel을 갖는 $ h \\times w $ feature map $ \\mathbf{h} $ 을 생각하고 1×1 kernel $ \\mathbf{W} $를 고려하면 다음과 같이 쉽게 Jacobian을 구할 수 있음  \n",
    "\n",
    "$$ \\log{|\\det{(\\frac{d \\, \\text{conv2D}(\\mathbf{h}; \\mathbf{W})}{d \\, \\mathbf{h}})}|} = h \\cdot w \\cdot \\log{|\\det(\\mathbf{W})|}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가로 저자들은 $ \\det(\\mathbf{W}) $ 의 연산량인 $ \\mathcal{O}(c^3) $ 를 줄이기 위해서 $ \\mathbf{W} $ 의 **LU decompotion**을 학습하는 방법으로 $ \\mathbf{W} $ 를 학습  \n",
    "LU decomposition을 사용하는 것으로 permutation matrix $ P $를 통해 $ \\mathbf{W} = \\mathbf{PL}(\\mathbf{U} + \\text{diag}(\\mathbf{s})) $ 로 표현할 수 있어짐  \n",
    "이는 결과적으로 $ \\log{|\\det(\\mathbf{W})|} = \\text{sum}(\\log{|\\mathbf{s}|}) $ 라 단순한 vector sum으로 구할 수 있어지므로 연산량을 $ \\mathcal{O}(c) $ 로 줄일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 RealNVP에 존재하던 Batch Normalization을 각 channel마다 수행하는 **Activation Normalization**으로 대체한 하나의 block을 구성  \n",
    "이러한 block을 여러번의 stack을 통하여 최종적인 model architecture로 구성함\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/86907286/181778533-53389880-928d-496e-b94f-c48a96ee24df.png\" alt=\"3\" width=\"400px\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxPFMMWQKOEY"
   },
   "source": [
    "## Implementation Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-w2YtibfT8Zy"
   },
   "source": [
    "+ paper의 중요 contribution인 [**1×1 Convolutions**](https://github.com/ikostrikov/pytorch-flows/blob/master/flows.py) implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LUInvertibleMM(nn.Module):\n",
    "    \"\"\" An implementation of a invertible matrix multiplication\n",
    "    layer from Glow: Generative Flow with Invertible 1x1 Convolutions\n",
    "    (https://arxiv.org/abs/1807.03039).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs):\n",
    "        super(LUInvertibleMM, self).__init__()\n",
    "        self.W = torch.Tensor(num_inputs, num_inputs)\n",
    "        nn.init.orthogonal_(self.W)\n",
    "        self.L_mask = torch.tril(torch.ones(self.W.size()), -1)\n",
    "        self.U_mask = self.L_mask.t().clone()\n",
    "\n",
    "        P, L, U = sp.linalg.lu(self.W.numpy())\n",
    "        self.P = torch.from_numpy(P)\n",
    "        self.L = nn.Parameter(torch.from_numpy(L))\n",
    "        self.U = nn.Parameter(torch.from_numpy(U))\n",
    "\n",
    "        S = np.diag(U)\n",
    "        sign_S = np.sign(S)\n",
    "        log_S = np.log(abs(S))\n",
    "        self.sign_S = torch.from_numpy(sign_S)\n",
    "        self.log_S = nn.Parameter(torch.from_numpy(log_S))\n",
    "\n",
    "        self.I = torch.eye(self.L.size(0))\n",
    "\n",
    "    def forward(self, inputs, cond_inputs=None, mode='direct'):\n",
    "        if str(self.L_mask.device) != str(self.L.device):\n",
    "            self.L_mask = self.L_mask.to(self.L.device)\n",
    "            self.U_mask = self.U_mask.to(self.L.device)\n",
    "            self.I = self.I.to(self.L.device)\n",
    "            self.P = self.P.to(self.L.device)\n",
    "            self.sign_S = self.sign_S.to(self.L.device)\n",
    "\n",
    "        L = self.L * self.L_mask + self.I\n",
    "        U = self.U * self.U_mask + torch.diag(\n",
    "            self.sign_S * torch.exp(self.log_S))\n",
    "        W = self.P @ L @ U\n",
    "\n",
    "        if mode == 'direct':\n",
    "            return inputs @ W, self.log_S.sum().unsqueeze(0).unsqueeze(\n",
    "                0).repeat(inputs.size(0), 1)\n",
    "        else:\n",
    "            return inputs @ torch.inverse(\n",
    "                W), -self.log_S.sum().unsqueeze(0).unsqueeze(0).repeat(\n",
    "                    inputs.size(0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ak0Dn-LURm2"
   },
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahcpUlB9UUpB"
   },
   "source": [
    "https://arxiv.org/pdf/1807.03039.pdf  \n",
    "https://arxiv.org/pdf/1505.05770.pdf  \n",
    "https://arxiv.org/pdf/2011.13456.pdf  \n",
    "https://github.com/ikostrikov/pytorch-flows/blob/master/flows.py  \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOy6sMktL3Wiz8Wfsn1WpYv",
   "collapsed_sections": [
    "uxPFMMWQKOEY",
    "-w2YtibfT8Zy",
    "5g9CBFXgUuNR",
    "7hkeM1CRU3W0"
   ],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
