{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdslyxQZbt4e"
   },
   "source": [
    "# 배경지식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxomXiYNaNW_"
   },
   "source": [
    "## Change of Variable Theorem\n",
    "\n",
    "$z$ ~ $\\pi (z)$ # given a random variable z and its know pdf\n",
    "\n",
    "$z = f^{-1}(x)$     $x = f(z)$ # new random variable using a 1 : 1 mapping funciton\n",
    "\n",
    "$p(x) = \\pi(z)|\\frac{dz}{dx}| = \\pi(f^{-1}(x))|(f^{-1})'(x)|$ # at single variable\n",
    "\n",
    "$p(X) = \\pi(Z)|det(\\frac{dZ}{dX})|$ # at multivariable # 뒤에 붙은 Jacobian Matrix가 각 변수들의 변화량에 관련된 정보들을 담고 있음\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcVwfQnPb-wH"
   },
   "source": [
    "## Normalizing Flows\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296144-d6d6a1ba-64da-48f9-8211-8b86618d8f01.png\" width=\"500\"/>\n",
    "\n",
    "VAE와는 달리 Encoder, Decoder 함수가 별개의 함수가 아니라 f(x)와 그 역함수로 구성\n",
    "\n",
    "z의 distribution을 구하면 p(x)도 구할 수 있음\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296149-b4677b26-659a-48be-b6e1-2d5bd61043c6.jpg\" width=\"500\"/>\n",
    "\n",
    "*Change of Variable Theorem 과 마찬가지의 방법으로 식 도출*\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296148-d52fed58-8ae1-4c16-bfd0-3b6781645176.jpg\" width=\"500\"/>\n",
    "\n",
    "식의 변형과정\n",
    "\n",
    "(1) → (2) 과정 : $\\frac{df_{i}^{-1}}{dz_i} = \\frac{dz_{i-1}}{dz_i} = (\\frac{dz_i}{dz_{i-1}})^{-1} = (\\frac{df_i}{dz_{i-1}})^{-1}$\n",
    "\n",
    "(2) → (3) 과정 : $det(M)det(M^{-1}) = det(I) = 1~~~~~\\therefore det(M^{-1}) = \\frac{1}{det(M)} = det(M)^{-1}$\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296147-69a68cf5-5ad8-4431-90e6-2bd3314c5f8a.jpg\" width=\"500\"/>\n",
    "\n",
    "반복적인 연산을 통해 최종 식 도출\n",
    "\n",
    "$\\therefore$ f의 jacobian의 determinant를 쉽게 구할 수 있어야된다. f가 역함수를 가져야 한다.\n",
    "\n",
    "# NICE: Non-linear Independent Components Estimation 리뷰\n",
    "\n",
    "## Abstract\n",
    "\n",
    "NICE ← deep learning framework modeling complex high-dimensional densities\n",
    "\n",
    "good representation ← the data has a distribution that is easy to model\n",
    "\n",
    "latent space로 mapping하는 non-linear deterministic transformation을 학습 → 위의 두 조건 만족시키면서\n",
    "\n",
    "criterion ← exact log-likelihood. tractable!\n",
    "\n",
    "## Introduction\n",
    "\n",
    "a good representation ← the distribution of the data is easy to model\n",
    "\n",
    "find a transformation *h = f(x)*\n",
    "\n",
    "resulting distribution(new pdf) factorizes \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296143-99aee7b4-a74f-4aae-9e99-22b7dd8b5352.png\" width=\"300\"/>\n",
    "\n",
    "Change of Variable Theorem에 따라 \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296140-2674d40c-1919-43ef-9e0b-5fe4343810fc.png\" width=\"300\"/>\n",
    "\n",
    "첫 설명과 식이 다른 이유 : 첫 설명 때는 x = f(h) 꼴이라서 함수의 방향이 반대일 뿐\n",
    "\n",
    "> In this paper, we choose f such that the determinant of the Jacobian is trivially obtained. Moreover, its inverse $f^{-1}$is also trivially obtained\n",
    "> \n",
    "\n",
    "core idea ← split x into two blocks (x1, x2) and apply as building block a transformation from (x1, x2) to (y1, y2). ***Coupling Layer***\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296138-f475ed1a-7f78-420c-962b-52f45c46e720.png\" width=\"300\"/>\n",
    "\n",
    "m은 임의의 complex function (논문의 실험에서는 ReLU MLP) → deep learning function 사용 가능\n",
    "\n",
    "## Learning Bijective Transformations of Continuous Probabilities\n",
    "\n",
    "maximum likelihood를 이용하기 위해 \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296132-0d403c69-34f3-46df-ab2b-5b4aefc0d7c3.png\" width=\"500\"/>\n",
    "\n",
    "Introduction의 첫 식을 이용하여 식 변형\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296128-b66b3844-ec74-4d1f-90a9-cbaf5d818d9a.png\" width=\"500\"/>\n",
    "\n",
    "NICE → learning invertible preprocessing transform of the dataset → increase likelihood arbitrarily \n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296137-8e5a095f-9662-4557-8949-221fde59a494.png\" width=\"500\"/>\n",
    "\n",
    "Computational graph of a coupling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDcJRbGooBTD"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "### Triangular Structure\n",
    "\n",
    "build a neural network with triangular weight matrices and bijective activation functions \n",
    "\n",
    "→ limiting design choices to depth and selection of non-linearities \n",
    "\n",
    "→ consider a family of functions with triangular Jacobian\n",
    "\n",
    "→ determinant of the Jacobian is also made easy to compute\n",
    "\n",
    "### Coupling Layer\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296136-2450901c-d83f-47d0-a33d-af8b7864cf05.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296135-14abe03c-9c62-4c2d-983a-b8294d7baa7f.png\" width=\"500\"/>\n",
    "<br>\n",
    "<img src=\"https://user-images.githubusercontent.com/66329748/160296133-9edbe5fb-9f8d-40d0-9063-dff0b461e618.png\" width=\"500\"/>\n",
    "\n",
    "there is no restriction placed on the choice of coupling function $m$\n",
    "\n",
    "g를 단순 additive coupling law로 만들면 계산이 매우 쉬워짐 → det()값이 1이 되어버림\n",
    "\n",
    "### Allowing Rescaling\n",
    "\n",
    "특정 dimension에 weight을 더 주고 다른 dimension에는 weight을 덜 주는 역할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlxSzDOLfLKK"
   },
   "source": [
    "# NICE 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOFvqSoj3LGs"
   },
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5heDwPLQ3AKb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.distributions import Distribution, Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PP1AkPe3sHd"
   },
   "source": [
    "## configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_-63uLh3tgv"
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "  'MODEL_SAVE_PATH': './saved_models/',\n",
    "\n",
    "  'USE_CUDA': True,\n",
    "\n",
    "  'TRAIN_BATCH_SIZE': 256,\n",
    "\n",
    "  'TRAIN_EPOCHS': 75,\n",
    "\n",
    "  'NUM_COUPLING_LAYERS': 4,\n",
    "\n",
    "  'NUM_NET_LAYERS': 6,  # neural net layers for each coupling layer\n",
    "\n",
    "  'NUM_HIDDEN_UNITS': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1Y_-_gk3eKw"
   },
   "source": [
    "## modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVbQOAGi3AN0"
   },
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Implementation of the additive coupling layer from section 3.2 of the NICE\n",
    "  paper.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, data_dim, hidden_dim, mask, num_layers=4):\n",
    "    super().__init__()\n",
    "\n",
    "    assert data_dim % 2 == 0\n",
    "\n",
    "    self.mask = mask\n",
    "\n",
    "    modules = [nn.Linear(data_dim, hidden_dim), nn.LeakyReLU(0.2)]\n",
    "    for i in range(num_layers - 2):\n",
    "      modules.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "      modules.append(nn.LeakyReLU(0.2))\n",
    "    modules.append(nn.Linear(hidden_dim, data_dim))\n",
    "\n",
    "    self.m = nn.Sequential(*modules)\n",
    "\n",
    "  def forward(self, x, logdet, invert=False):\n",
    "    if not invert:\n",
    "      x1, x2 = self.mask * x, (1. - self.mask) * x\n",
    "      y1, y2 = x1, x2 + (self.m(x1) * (1. - self.mask))\n",
    "      return y1 + y2, logdet\n",
    "\n",
    "    # Inverse additive coupling layer\n",
    "    y1, y2 = self.mask * x, (1. - self.mask) * x\n",
    "    x1, x2 = y1, y2 - (self.m(y1) * (1. - self.mask))\n",
    "    return x1 + x2, logdet\n",
    "\n",
    "\n",
    "class ScalingLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Implementation of the scaling layer from section 3.3 of the NICE paper.\n",
    "  \"\"\"\n",
    "  def __init__(self, data_dim):\n",
    "    super().__init__()\n",
    "    self.log_scale_vector = nn.Parameter(torch.randn(1, data_dim, requires_grad=True))\n",
    "\n",
    "  def forward(self, x, logdet, invert=False):\n",
    "    log_det_jacobian = torch.sum(self.log_scale_vector)\n",
    "\n",
    "    if invert:\n",
    "        return torch.exp(- self.log_scale_vector) * x, logdet - log_det_jacobian\n",
    "\n",
    "    return torch.exp(self.log_scale_vector) * x, logdet + log_det_jacobian\n",
    "\n",
    "\n",
    "class LogisticDistribution(Distribution):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def log_prob(self, x):\n",
    "    return -(F.softplus(x) + F.softplus(-x))\n",
    "\n",
    "  def sample(self, size):\n",
    "    if cfg['USE_CUDA']:\n",
    "      z = Uniform(torch.cuda.FloatTensor([0.]), torch.cuda.FloatTensor([1.])).sample(size)\n",
    "    else:\n",
    "      z = Uniform(torch.FloatTensor([0.]), torch.FloatTensor([1.])).sample(size)\n",
    "\n",
    "    return torch.log(z) - torch.log(1. - z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRc5kuAC33oc"
   },
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpWji8NP3AQZ"
   },
   "outputs": [],
   "source": [
    "class NICE(nn.Module):\n",
    "  def __init__(self, data_dim, num_coupling_layers=3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.data_dim = data_dim\n",
    "\n",
    "    # alternating mask orientations for consecutive coupling layers\n",
    "    masks = [self._get_mask(data_dim, orientation=(i % 2 == 0))\n",
    "                                            for i in range(num_coupling_layers)]\n",
    "\n",
    "    self.coupling_layers = nn.ModuleList([CouplingLayer(data_dim=data_dim,\n",
    "                                hidden_dim=cfg['NUM_HIDDEN_UNITS'],\n",
    "                                mask=masks[i], num_layers=cfg['NUM_NET_LAYERS'])\n",
    "                              for i in range(num_coupling_layers)])\n",
    "\n",
    "    self.scaling_layer = ScalingLayer(data_dim=data_dim)\n",
    "\n",
    "    self.prior = LogisticDistribution()\n",
    "\n",
    "  def forward(self, x, invert=False):\n",
    "    if not invert:\n",
    "      z, log_det_jacobian = self.f(x)\n",
    "      log_likelihood = torch.sum(self.prior.log_prob(z), dim=1) + log_det_jacobian\n",
    "      return z, log_likelihood\n",
    "\n",
    "    return self.f_inverse(x)\n",
    "\n",
    "  def f(self, x):\n",
    "    z = x\n",
    "    log_det_jacobian = 0\n",
    "    for i, coupling_layer in enumerate(self.coupling_layers):\n",
    "      z, log_det_jacobian = coupling_layer(z, log_det_jacobian)\n",
    "    z, log_det_jacobian = self.scaling_layer(z, log_det_jacobian)\n",
    "    return z, log_det_jacobian\n",
    "\n",
    "  def f_inverse(self, z):\n",
    "    x = z\n",
    "    x, _ = self.scaling_layer(x, 0, invert=True)\n",
    "    for i, coupling_layer in reversed(list(enumerate(self.coupling_layers))):\n",
    "      x, _ = coupling_layer(x, 0, invert=True)\n",
    "    return x\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    z = self.prior.sample([num_samples, self.data_dim]).view(self.samples, self.data_dim)\n",
    "    return self.f_inverse(z)\n",
    "\n",
    "  def _get_mask(self, dim, orientation=True):\n",
    "    mask = np.zeros(dim)\n",
    "    mask[::2] = 1.\n",
    "    if orientation:\n",
    "      mask = 1. - mask     # flip mask orientation\n",
    "    mask = torch.tensor(mask)\n",
    "    if cfg['USE_CUDA']:\n",
    "      mask = mask.cuda()\n",
    "    return mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO7q6I_e4ZUJ"
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xe-Z9sZb3ASw",
    "outputId": "1cd5518e-b114-44b2-d021-bf476ae56a50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class '__main__.LogisticDistribution'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  'with `validate_args=False` to turn off validation.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed. Log Likelihood: -1046.8690185546875\n",
      "Epoch 1 completed. Log Likelihood: -854.7686767578125\n",
      "Epoch 2 completed. Log Likelihood: -672.8143920898438\n",
      "Epoch 3 completed. Log Likelihood: -494.6536560058594\n",
      "Epoch 4 completed. Log Likelihood: -320.71441650390625\n",
      "Epoch 5 completed. Log Likelihood: -151.33949279785156\n",
      "Epoch 6 completed. Log Likelihood: 12.598835945129395\n",
      "Epoch 7 completed. Log Likelihood: 170.87037658691406\n",
      "Epoch 8 completed. Log Likelihood: 322.28997802734375\n",
      "Epoch 9 completed. Log Likelihood: 466.75970458984375\n",
      "Epoch 10 completed. Log Likelihood: 603.1981201171875\n",
      "Epoch 11 completed. Log Likelihood: 731.8975830078125\n",
      "Epoch 12 completed. Log Likelihood: 852.1170043945312\n",
      "Epoch 13 completed. Log Likelihood: 964.4251098632812\n",
      "Epoch 14 completed. Log Likelihood: 1068.567138671875\n",
      "Epoch 15 completed. Log Likelihood: 1165.2664794921875\n",
      "Epoch 16 completed. Log Likelihood: 1254.1492919921875\n",
      "Epoch 17 completed. Log Likelihood: 1335.5655517578125\n",
      "Epoch 18 completed. Log Likelihood: 1410.65087890625\n",
      "Epoch 19 completed. Log Likelihood: 1477.23291015625\n",
      "Epoch 20 completed. Log Likelihood: 1538.2586669921875\n",
      "Epoch 21 completed. Log Likelihood: 1594.236572265625\n",
      "Epoch 22 completed. Log Likelihood: 1641.1064453125\n",
      "Epoch 23 completed. Log Likelihood: 1683.7479248046875\n",
      "Epoch 24 completed. Log Likelihood: 1721.6748046875\n",
      "Epoch 25 completed. Log Likelihood: 1752.8677978515625\n",
      "Epoch 26 completed. Log Likelihood: 1779.13623046875\n",
      "Epoch 27 completed. Log Likelihood: 1804.3917236328125\n",
      "Epoch 28 completed. Log Likelihood: 1825.754150390625\n",
      "Epoch 29 completed. Log Likelihood: 1842.5098876953125\n",
      "Epoch 30 completed. Log Likelihood: 1857.39306640625\n",
      "Epoch 31 completed. Log Likelihood: 1873.1177978515625\n",
      "Epoch 32 completed. Log Likelihood: 1884.1175537109375\n",
      "Epoch 33 completed. Log Likelihood: 1892.7962646484375\n",
      "Epoch 34 completed. Log Likelihood: 1905.513427734375\n",
      "Epoch 35 completed. Log Likelihood: 1914.781005859375\n",
      "Epoch 36 completed. Log Likelihood: 1921.36572265625\n",
      "Epoch 37 completed. Log Likelihood: 1929.6881103515625\n",
      "Epoch 38 completed. Log Likelihood: 1934.612060546875\n",
      "Epoch 39 completed. Log Likelihood: 1942.163330078125\n",
      "Epoch 40 completed. Log Likelihood: 1945.5450439453125\n",
      "Epoch 41 completed. Log Likelihood: 1949.83349609375\n",
      "Epoch 42 completed. Log Likelihood: 1959.077880859375\n",
      "Epoch 43 completed. Log Likelihood: 1961.4378662109375\n",
      "Epoch 44 completed. Log Likelihood: 1965.7811279296875\n",
      "Epoch 45 completed. Log Likelihood: 1968.2996826171875\n",
      "Epoch 46 completed. Log Likelihood: 1970.0081787109375\n",
      "Epoch 47 completed. Log Likelihood: 1977.220458984375\n",
      "Epoch 48 completed. Log Likelihood: 1982.3223876953125\n",
      "Epoch 49 completed. Log Likelihood: 1984.2391357421875\n",
      "Epoch 50 completed. Log Likelihood: 1986.1912841796875\n",
      "Epoch 51 completed. Log Likelihood: 1989.4842529296875\n",
      "Epoch 52 completed. Log Likelihood: 1994.020263671875\n",
      "Epoch 53 completed. Log Likelihood: 1995.14453125\n",
      "Epoch 54 completed. Log Likelihood: 1997.627685546875\n",
      "Epoch 55 completed. Log Likelihood: 1999.98876953125\n",
      "Epoch 56 completed. Log Likelihood: 2004.4227294921875\n",
      "Epoch 57 completed. Log Likelihood: 2005.478271484375\n",
      "Epoch 58 completed. Log Likelihood: 2007.2034912109375\n",
      "Epoch 59 completed. Log Likelihood: 2010.6297607421875\n",
      "Epoch 60 completed. Log Likelihood: 2011.921875\n",
      "Epoch 61 completed. Log Likelihood: 2014.18505859375\n",
      "Epoch 62 completed. Log Likelihood: 2015.98388671875\n",
      "Epoch 63 completed. Log Likelihood: 2019.634033203125\n",
      "Epoch 64 completed. Log Likelihood: 2019.2713623046875\n",
      "Epoch 65 completed. Log Likelihood: 2021.083740234375\n",
      "Epoch 66 completed. Log Likelihood: 2022.615478515625\n",
      "Epoch 67 completed. Log Likelihood: 2025.210693359375\n",
      "Epoch 68 completed. Log Likelihood: 2025.7947998046875\n",
      "Epoch 69 completed. Log Likelihood: 2027.5740966796875\n",
      "Epoch 70 completed. Log Likelihood: 2030.6951904296875\n",
      "Epoch 71 completed. Log Likelihood: 2030.0704345703125\n",
      "Epoch 72 completed. Log Likelihood: 2033.8212890625\n",
      "Epoch 73 completed. Log Likelihood: 2034.4552001953125\n",
      "Epoch 74 completed. Log Likelihood: 2034.9815673828125\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root='./data/mnist', train=True, transform=transform, download=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=cfg['TRAIN_BATCH_SIZE'],\n",
    "                                         shuffle=True, pin_memory=True)\n",
    "\n",
    "model = NICE(data_dim=784, num_coupling_layers=cfg['NUM_COUPLING_LAYERS'])\n",
    "if cfg['USE_CUDA']:\n",
    "  device = torch.device('cuda')\n",
    "  model = model.to(device)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "\n",
    "opt = optim.Adam(model.parameters())\n",
    "\n",
    "for i in range(cfg['TRAIN_EPOCHS']):\n",
    "  mean_likelihood = 0.0\n",
    "  num_minibatches = 0\n",
    "\n",
    "  for batch_id, (x, _) in enumerate(dataloader):\n",
    "      x = x.view(-1, 784) + torch.rand(784) / 256.\n",
    "      if cfg['USE_CUDA']:\n",
    "        x = x.cuda()\n",
    "\n",
    "      x = torch.clamp(x, 0, 1)\n",
    "\n",
    "      z, likelihood = model(x)\n",
    "      loss = -torch.mean(likelihood)   # NLL\n",
    "\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      model.zero_grad()\n",
    "\n",
    "      mean_likelihood -= loss\n",
    "      num_minibatches += 1\n",
    "\n",
    "  mean_likelihood /= num_minibatches\n",
    "  print('Epoch {} completed. Log Likelihood: {}'.format(i, mean_likelihood))\n",
    "\n",
    "  # if i % 5 == 0:\n",
    "    # save_path = os.path.join(cfg['MODEL_SAVE_PATH'], '{}.pt'.format(i))\n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "    # print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBDinNHO62IS"
   },
   "source": [
    "### 논문에서 MNIST Dataset 기준 Log-likelihood는 1980.50"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rdslyxQZbt4e",
    "6cC2iJiRnkbG",
    "wlxSzDOLfLKK",
    "bOFvqSoj3LGs",
    "6PP1AkPe3sHd",
    "m1Y_-_gk3eKw",
    "kRc5kuAC33oc",
    "CO7q6I_e4ZUJ"
   ],
   "name": "NICE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
